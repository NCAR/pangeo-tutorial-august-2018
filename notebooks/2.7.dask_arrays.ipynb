{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6. Dask Array\n",
    "\n",
    "![](dask-array-black-text.svg)\n",
    "\n",
    "*From the Dask documentation:*\n",
    "> Dask Array implements a subset of the NumPy ndarray interface using \n",
    "> blocked algorithms, cutting up the large array into many small arrays. \n",
    "> This lets us compute on arrays larger than memory using all of our cores. \n",
    "> We coordinate these blocked algorithms using dask graphs.\n",
    "\n",
    "Dask Arrays provide \"a parallel, larger-than-memory, n-dimensional array using blocked algorithms. Simply put: distributed Numpy.\n",
    "\n",
    "- **Parallel:** Uses all of the cores on your computer\n",
    "- **Larger-than-memory:** Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n",
    "- **Blocked Algorithms:** Perform large computations by performing many smaller computations\"\n",
    "\n",
    "*Stolen from the Dask tutorial: https://github.com/dask/dask-tutorial*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "cluster = LocalCluster(n_workers=4)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import netCDF4 as nc4\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dask Arrays from Numpy Arrays\n",
    "\n",
    "One of the easiest ways of creating Dask Arrays is directly from Numpy arrays using the `from_array` function of `dask.array`.  This function accepts anything that is \"array-like,\" such as:\n",
    "\n",
    "- a netCDF4 variable from netCDF4-python,\n",
    "- an HDF5 field using h5py,\n",
    "- a Numpy array,\n",
    "\n",
    "or any other object that can be indexed like a Numpy array.\n",
    "\n",
    "It takes the \"array-like\" object and a tuple for the `chunks` parameter, which specifies the size of each chunk of the array along each array axis.\n",
    "\n",
    "In this example, we'll create a distributed Dask Array from NetCDF file...but first we have to create some NetCDF data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(4):\n",
    "    i, j = divmod(n, 2)\n",
    "    ncds = nc4.Dataset('data-{}x{}.nc'.format(i, j), 'w')\n",
    "    ncds.createDimension('x', 5120)\n",
    "    ncds.createDimension('y', 5120)\n",
    "    x = ncds.createVariable('x', 'd', ('x',))\n",
    "    y = ncds.createVariable('y', 'd', ('y',))\n",
    "    v = ncds.createVariable('v', 'f', ('x', 'y'))\n",
    "    u = ncds.createVariable('u', 'f', ('x', 'y'))\n",
    "    x[:] = np.arange(i*5120, (i+1)*5120)\n",
    "    y[:] = np.arange(j*5120, (j+1)*5120)\n",
    "    v[:] = np.random.random((5120,5120)).astype('f')\n",
    "    u[:] = np.random.random((5120,5120)).astype('f')\n",
    "    ncds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lh data*.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncds = nc4.Dataset('data-0x0.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_da = da.from_array(ncds.variables['v'], chunks=(2560, 2560))\n",
    "v_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_da = da.from_array(ncds.variables['u'], chunks=(2560,2560))\n",
    "u_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, take a look at the Dashboard, again.**  Look at the *Bytes stored* section of the Dashboard Status page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_da.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_da.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_da.numblocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dask Arrays from Delayed Objects\n",
    "\n",
    "In addition to reading array-like objects as Dasy Arrays, you can construct an array from `Delayed` objects, giving you a little more flexibility in what you can construct an array from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
