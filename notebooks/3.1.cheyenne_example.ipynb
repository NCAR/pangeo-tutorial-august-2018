{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. A Cheyenne Example\n",
    "\n",
    "This example is meant to use one of the RDA datasets available on Cheyenne's GLADE storage system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Set Up Your Dask Distributed Cluster on Cheyenne\n",
    "\n",
    "The easiest way to start a cluster on Cheyenne is using the `dask.distributed` `Client` object and a `PBSCluster` object from the `dask-jobqueue` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "cluster = PBSCluster(queue='regular', project='NIOW0001', interface='ib0',\n",
    "                     processes=36, cores=36, memory='108GB', walltime='01:00:00')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PBSCluster\n",
    "\n",
    "The `PBSCluster` object defines what a \"single worker\" looks like on a PBS-based job queuing system, like the one on NCAR's Cheyenne.  Each \"worker\" is submitted to the PBS queuing system separately, as its own job.  So, the arguments of the `PBSCluster` object define what is requested for each \"worker\" via the PBS queuing system:\n",
    "\n",
    "  - `queue`: The job queue to worker job is submitted to.\n",
    "  - `project`: The project ID code for your allocation on Cheyenne.\n",
    "  - `processes`: The number of separate Python processes associated with a single worker job.\n",
    "  - `cores`: The number of cores given to a single worker.\n",
    "  - `memory`: The memory available to a single worker.\n",
    "  - `walltime`: The walltime to give to the worker job.\n",
    "  \n",
    "The setup above initializes a cluster with 0 nodes.  No jobs are submitted to the PBS queue, yet.  However, once we submit jobs to the PBS queue (with the `scale` command), each job will contain 9 processes (or workers) and 4 cores-per-process (for 36 cores total, or a full Cheyenne node) for 1 hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up your cluster\n",
    "\n",
    "Now that we have the cluster, and we have the SSH tunnel open to the dashboard, we can request some workers using the `scale` command of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you rerun the `client` command above, you will eventually notice the number of worker rise to 36 (and the number of cores rise to 144).  By submitted 4 PBS jobs to the queue, we eventually launched 36 Dask workers, with each worker having 4 cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard\n",
    "\n",
    "In the above output (of the `client` object), we see the number of workers and the total number of cores in the cluster.  Initially, we see 0 workers (and 0 cores, obviously).  Eventually, when your PBS jobs start, you should see 36 workers (144 cores).\n",
    "\n",
    "We also see a pointer to the Dashboard.  Unfortunately, because we are accessing this cluster via an SSH tunnel, we cannot click this link and be taken to the Dashboard directly.  Instead, we have to note the Dashboard port number (probably 8787) and create *another* SSH tunnel to redirect that port to our browser.\n",
    "\n",
    "In a termal, do that now.  It should look exactly like the other SSH tunnel command, but with the port numbers changed to match the port of the above link.  Then, open up another browser tab with *localhost:port*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: *Sea Surface Altimetry Data Analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "plt.rcParams['figure.figsize'] = (15,10)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens_ocn_ssh_daily_rcp85 = '/glade/p_old/cesmLE/CESM-CAM5-BGC-LE/ocn/proc/tseries/daily/SSH_2/b.e11.BRCP85C5CNBDRD.f09_g16.*.pop.h.nday1.SSH_2.20060102-20801231.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(lens_ocn_ssh_daily_rcp85, decode_times=False, chunks={'time':100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Metadata\n",
    "For those unfamiliar with this dataset, the variable metadata is very helpful for understanding what the variables actually represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in sorted(ds.data_vars):\n",
    "    print('{:>20}: {}'.format(v, ds[v].attrs.get('long_name', '????????????????????????????????????')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look a little deeper at the Sea Surface Height (SSH) Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds.SSH_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds.SSH_2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.SSH_2.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.SSH_2.data.numblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.SSH_2.data.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually Examine Some of the Data\n",
    "Let's do a sanity check that the sea surface height (SSH) data looks reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.SSH_2.sel(time=897.0, method='nearest').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How big is the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.nbytes / 2**40, 'TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.SSH_2.nbytes / 2**20, 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeseries of Global Mean Sea Level\n",
    "Here we make a simple yet fundamental calculation: the rate of increase of global mean sea level over the observational period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_timeseries = ds.SSH_2.mean(dim=('nlat', 'nlon')).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_rolling_annual_mean = ssh_timeseries.rolling(time=365, center=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_timeseries.plot(label='full data')\n",
    "ssh_rolling_annual_mean.plot(label='rolling annual mean')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sea Level Variability\n",
    "We can examine the natural variability in sea level by looking at its standard deviation in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_std = ds.SSH_2.std(dim='time').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_std.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ssh_std.plot.contourf(ax=ax, transform=ccrs.PlateCarree())\n",
    "ax.set_global()\n",
    "ax.coastlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
